{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crf import CRF\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "import constant as config\n",
    "class BERT_CRF_Joint(nn.Module):\n",
    "    def __init__(self, config=config, bert=None, distill=False):\n",
    "        super(BERT_CRF_Joint, self).__init__()\n",
    "        \n",
    "        \n",
    "        #별도의 BERT모델을 지정하지 않으면 SKT KoBERT를 Default로 지정한다. \n",
    "        self.bert = bert\n",
    "        self.distill=distill\n",
    "        if bert is None:\n",
    "            if self.distill == True:\n",
    "                self.bert = DistilBertModel.from_pretrained('monologg/distilkobert')\n",
    "            else:\n",
    "                self.bert, self.vocab  = get_pytorch_kobert_model()\n",
    "                \n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.crf_linear = nn.Linear(config.hidden_size, config.num_entity)\n",
    "        self.intent_classifier = nn.Linear(config.hidden_size, config.num_intent)\n",
    "        self.bilstm  = nn.LSTM(config.hidden_size, config.hidden_size //2, \n",
    "                               batch_first=True, bidirectional=True )\n",
    "        self.crf = CRF(num_tags=config.num_entity, batch_first=True)\n",
    "    \n",
    "    \n",
    "    #Sentence의 길이만큼만 Attention을 취하기 위해 Mask를 생성한다.\n",
    "    def get_attention_mask(self, input_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, input_ids, valid_length, token_type_ids, entity=None, intent=None):\n",
    "        attention_mask = self.get_attention_mask(input_ids, valid_length)\n",
    "        \n",
    "        #all_encoder_layers는 BERT의 output representation 전부이고\n",
    "        #poold_output은 CLS token의 representation 값이다.\n",
    "        #기본 kobert와 distill kobert의 output형태가 다르기 때문에 분기처리하였다.\n",
    "        if self.distill==True:\n",
    "            outputs = self.bert(input_ids=input_ids.long(), \n",
    "                                attention_mask=attention_mask) \n",
    "            \n",
    "            all_encoder_layers, pooled_output = outputs[0], outputs[0][:,0,:]\n",
    "            \n",
    "        else:\n",
    "            all_encoder_layers, pooled_output = self.bert(input_ids=input_ids.long(),\n",
    "                                                      token_type_ids=token_type_ids,\n",
    "                                                      attention_mask=attention_mask)\n",
    "\n",
    "        cls_out = pooled_output\n",
    "        cls_out_drop = self.dropout(cls_out)\n",
    "        logits = self.intent_classifier(cls_out_drop)\n",
    "        \n",
    "        # Entity on CRF\n",
    "        last_encoder_layer = all_encoder_layers\n",
    "        drop = self.dropout(last_encoder_layer)\n",
    "        output, hc = self.bilstm(drop)\n",
    "        linear = self.crf_linear(output)\n",
    "        tag_seq = self.crf.decode(linear)\n",
    "\n",
    "        # For training\n",
    "        if entity is not None:\n",
    "            log_likelihood = self.crf(linear, entity)       \n",
    "            return log_likelihood, tag_seq, logits\n",
    "        \n",
    "        # For inference\n",
    "        else: \n",
    "            confidence = self.crf.compute_confidence(linear, tag_seq)\n",
    "            return tag_seq, confidence, logits\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    "batch=[]\n",
    "valid_length=[]\n",
    "batch.append(tokenizer.encode(\"안녕하세요. 반갑습니다. 티맥스입니다. 테스트코드를 작성합니다.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"안녕하세요. 반갑습니다. 티맥스입니다. 테스트코드를 작성합니다.\")))\n",
    "batch.append(tokenizer.encode(\"오늘은 버트를 이용하여 테스트를 해보겠습니다.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"오늘은 버트를 이용하여 테스트를 해보겠습니다.\")))\n",
    "batch.append(tokenizer.encode(\"조인트 모델이 잘 돌아가는지 궁금하군요.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"조인트 모델이 잘 돌아가는지 궁금하군요.\")))\n",
    "\n",
    "\n",
    "maxlen=max(valid_length)\n",
    "input_ids = [sen+[0]*(maxlen-len(sen)) for sen in batch]\n",
    "input_ids= torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = BERT_CRF_Joint(distill=True)\n",
    "joint(input_ids, valid_length, token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
