{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crf import CRF\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "import constant as config\n",
    "class BERT_CRF_Joint(nn.Module):\n",
    "    def __init__(self, config=config, bert=None, distill=False):\n",
    "        super(BERT_CRF_Joint, self).__init__()\n",
    "        \n",
    "        \n",
    "        #별도의 BERT모델을 지정하지 않으면 SKT KoBERT를 Default로 지정한다. \n",
    "        self.bert = bert\n",
    "        self.distill=distill\n",
    "        if bert is None:\n",
    "            if self.distill == True:\n",
    "                self.bert = DistilBertModel.from_pretrained('monologg/distilkobert')\n",
    "            else:\n",
    "                self.bert, self.vocab  = get_pytorch_kobert_model()\n",
    "                \n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.crf_linear = nn.Linear(config.hidden_size, config.num_entity)\n",
    "        self.intent_classifier = nn.Linear(config.hidden_size, config.num_intent)\n",
    "        self.bilstm  = nn.LSTM(config.hidden_size, config.hidden_size //2, \n",
    "                               batch_first=True, bidirectional=True )\n",
    "        self.crf = CRF(num_tags=config.num_entity, batch_first=True)\n",
    "    \n",
    "    \n",
    "    #Sentence의 길이만큼만 Attention을 취하기 위해 Mask를 생성한다.\n",
    "    def get_attention_mask(self, input_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, input_ids, valid_length, token_type_ids, entity=None, intent=None):\n",
    "        attention_mask = self.get_attention_mask(input_ids, valid_length)\n",
    "        \n",
    "        #all_encoder_layers는 BERT의 output representation 전부이고\n",
    "        #poold_output은 CLS token의 representation 값이다.\n",
    "        #기본 kobert와 distill kobert의 output형태가 다르기 때문에 분기처리하였다.\n",
    "        if self.distill==True:\n",
    "            outputs = self.bert(input_ids=input_ids.long(), \n",
    "                                attention_mask=attention_mask) \n",
    "            \n",
    "            all_encoder_layers, pooled_output = outputs[0], outputs[0][:,0,:]\n",
    "            \n",
    "        else:\n",
    "            all_encoder_layers, pooled_output = self.bert(input_ids=input_ids.long(),\n",
    "                                                      token_type_ids=token_type_ids,\n",
    "                                                      attention_mask=attention_mask)\n",
    "\n",
    "        cls_out = pooled_output\n",
    "        cls_out_drop = self.dropout(cls_out)\n",
    "        logits = self.intent_classifier(cls_out_drop)\n",
    "        \n",
    "        # Entity on CRF\n",
    "        last_encoder_layer = all_encoder_layers\n",
    "        drop = self.dropout(last_encoder_layer)\n",
    "        output, hc = self.bilstm(drop)\n",
    "        linear = self.crf_linear(output)\n",
    "        tag_seq = self.crf.decode(linear)\n",
    "\n",
    "        # For training\n",
    "        if entity is not None:\n",
    "            log_likelihood = self.crf(linear, entity)       \n",
    "            return log_likelihood, tag_seq, logits\n",
    "        \n",
    "        # For inference\n",
    "        else: \n",
    "            confidence = self.crf.compute_confidence(linear, tag_seq)\n",
    "            return tag_seq, confidence, logits\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    "batch=[]\n",
    "valid_length=[]\n",
    "batch.append(tokenizer.encode(\"안녕하세요. 반갑습니다. 테스트코드를 작성합니다.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"안녕하세요. 반갑습니다. 테스트코드를 작성합니다.\")))\n",
    "batch.append(tokenizer.encode(\"오늘은 버트를 이용하여 테스트를 해보겠습니다.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"오늘은 버트를 이용하여 테스트를 해보겠습니다.\")))\n",
    "batch.append(tokenizer.encode(\"조인트 모델이 잘 돌아가는지 궁금하군요.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"조인트 모델이 잘 돌아가는지 궁금하군요.\")))\n",
    "\n",
    "\n",
    "maxlen=max(valid_length)\n",
    "input_ids = [sen+[0]*(maxlen-len(sen)) for sen in batch]\n",
    "input_ids= torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[11, 17, 1, 212, 212, 212, 110, 219, 92, 1, 212, 118, 215, 53, 118, 212],\n",
       "  [212, 212, 118, 93, 92, 1, 212, 212, 212, 118, 194, 1, 212, 212, 212, 212],\n",
       "  [230, 73, 35, 212, 93, 241, 82, 93, 11, 1, 194, 56, 92, 191, 92, 183]],\n",
       " 0.00465765967965126,\n",
       " tensor([[-0.3264,  0.1196,  0.4184, -0.1342, -0.4970, -0.0875,  0.0306,  0.2577,\n",
       "           0.1859,  0.2623,  0.1183,  0.0730, -0.4708,  0.1127, -0.1290,  0.1809,\n",
       "           0.3338,  0.6026,  0.3228,  0.5081, -0.1881, -0.2677, -0.1861, -0.1454,\n",
       "           0.7420,  0.0959,  0.1700, -0.0903, -0.0798,  0.1567,  0.0990,  0.5753,\n",
       "           0.1216,  0.1859,  0.2001, -0.5072, -0.2218,  0.3213, -0.1307, -0.0891,\n",
       "           0.2631,  0.3012, -0.6117,  0.3088,  0.2558, -0.8699,  0.3474,  0.3200,\n",
       "           0.0498, -0.0896, -0.0058,  0.1389,  0.4176,  0.0896,  0.1912, -0.1358,\n",
       "           0.0468,  0.3852,  0.2406,  0.1795,  0.5275, -0.1891,  0.6130,  0.4632,\n",
       "          -0.1768,  0.1506,  0.0659, -0.0788, -0.2129,  0.0477, -0.3145, -0.0031],\n",
       "         [-0.0319, -0.3848,  0.2563, -0.0736, -0.1387,  0.2177, -0.1012,  0.1926,\n",
       "           0.2898,  0.0111,  0.1597,  0.1202, -0.5192,  0.1567,  0.0074,  0.1327,\n",
       "           0.2533,  0.1723,  0.1068,  0.4427, -0.1997, -0.3119,  0.2033, -0.2294,\n",
       "           0.6183, -0.0821,  0.3447, -0.4163,  0.0162,  0.0884, -0.1704,  0.3154,\n",
       "           0.0702,  0.1705,  0.0488, -0.2125,  0.1125,  0.2703, -0.2239, -0.0804,\n",
       "           0.4696,  0.3347, -0.6272,  0.3484,  0.3243, -0.6653,  0.1727, -0.0035,\n",
       "           0.0557, -0.4478, -0.0109,  0.2204,  0.4090, -0.0663,  0.0621,  0.1677,\n",
       "          -0.0960,  0.4004,  0.2235,  0.4140,  0.4451,  0.0376,  0.4787,  0.2233,\n",
       "          -0.0751, -0.0492,  0.0827, -0.0854,  0.1161, -0.0861, -0.0196,  0.2573],\n",
       "         [ 0.2854,  0.4061,  0.4111, -0.0099, -0.4897, -0.0427,  0.0349,  0.4013,\n",
       "           0.3321,  0.3760, -0.2582,  0.5213,  0.1700,  0.5994, -0.3865,  0.1543,\n",
       "           0.4170,  0.4072,  0.2561,  0.3819, -0.0501,  0.0892,  0.1458, -0.3911,\n",
       "           0.7858, -0.0020,  0.2850, -0.0697, -0.0033, -0.2020, -0.0165,  0.2689,\n",
       "           0.0215,  0.1523,  0.3468, -0.3198, -0.0422,  0.2975, -0.0732,  0.0073,\n",
       "           0.1953,  0.1111, -0.6839, -0.0660,  0.2459, -0.8896,  0.1996,  0.4633,\n",
       "           0.5018, -0.1875, -0.0822,  0.1399,  0.2071,  0.4379,  0.1104,  0.2079,\n",
       "          -0.1304,  0.4872, -0.0083,  0.0441,  0.2232, -0.0943,  0.4437,  0.3053,\n",
       "          -0.1211,  0.3424,  0.0685, -0.0270,  0.0132, -0.2019, -0.1990, -0.1876]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint = BERT_CRF_Joint(distill=True)\n",
    "joint(input_ids, valid_length, token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
