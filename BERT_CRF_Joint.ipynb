{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crf import CRF\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import DistilBertModel\n",
    "import config\n",
    "\n",
    "class BERT_CRF_Joint(nn.Module):\n",
    "    def __init__(self, config=config, bert=None, distill=False):\n",
    "        super(BERT_CRF_Joint, self).__init__()\n",
    "        \n",
    "        \n",
    "        #별도의 BERT모델을 지정하지 않으면 SKT KoBERT를 Default로 지정한다. \n",
    "        self.bert = bert\n",
    "        self.distill=distill\n",
    "        if bert is None:\n",
    "            if self.distill == True:\n",
    "                self.bert = DistilBertModel.from_pretrained('monologg/distilkobert')\n",
    "            else:\n",
    "                self.bert, self.vocab  = get_pytorch_kobert_model()\n",
    "                \n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.crf_linear = nn.Linear(config.hidden_size, config.num_entity)\n",
    "        self.intent_classifier = nn.Linear(config.hidden_size, config.num_intent)\n",
    "        self.bilstm  = nn.LSTM(config.hidden_size, config.hidden_size //2, \n",
    "                               batch_first=True, bidirectional=True )\n",
    "        self.crf = CRF(num_tags=config.num_entity, batch_first=True)\n",
    "    \n",
    "    \n",
    "    #Sentence의 길이만큼만 Attention을 취하기 위해 Mask를 생성한다.\n",
    "    def get_attention_mask(self, input_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, input_ids, valid_length, token_type_ids, entity=None, intent=None):\n",
    "        attention_mask = self.get_attention_mask(input_ids, valid_length)\n",
    "        \n",
    "        #all_encoder_layers는 BERT의 output representation 전부이고\n",
    "        #poold_output은 CLS token의 representation 값이다.\n",
    "        #기본 kobert와 distill kobert의 output형태가 다르기 때문에 분기처리하였다.\n",
    "        if self.distill==True:\n",
    "            outputs = self.bert(input_ids=input_ids.long(), \n",
    "                                attention_mask=attention_mask) \n",
    "            \n",
    "            all_encoder_layers, pooled_output = outputs[0], outputs[0][:,0,:]\n",
    "            \n",
    "        else:\n",
    "            all_encoder_layers, pooled_output = self.bert(input_ids=input_ids.long(),\n",
    "                                                      token_type_ids=token_type_ids,\n",
    "                                                      attention_mask=attention_mask)\n",
    "\n",
    "        cls_out = pooled_output\n",
    "        cls_out_drop = self.dropout(cls_out)\n",
    "        logits = self.intent_classifier(cls_out_drop)\n",
    "        \n",
    "        # Entity on CRF\n",
    "        last_encoder_layer = all_encoder_layers\n",
    "        drop = self.dropout(last_encoder_layer)\n",
    "        output, hc = self.bilstm(drop)\n",
    "        linear = self.crf_linear(output)\n",
    "        tag_seq = self.crf.decode(linear)\n",
    "\n",
    "        # For training\n",
    "        if entity is not None:\n",
    "            log_likelihood = self.crf(linear, entity)       \n",
    "            return log_likelihood, tag_seq, logits\n",
    "        \n",
    "        # For inference\n",
    "        else: \n",
    "            confidence = self.crf.compute_confidence(linear, tag_seq)\n",
    "            return tag_seq, confidence, logits\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    "batch=[]\n",
    "valid_length=[]\n",
    "batch.append(tokenizer.encode(\"안녕하세요. 반갑습니다. 테스트코드를 작성합니다.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"안녕하세요. 반갑습니다. 테스트코드를 작성합니다.\")))\n",
    "batch.append(tokenizer.encode(\"오늘은 버트를 이용하여 테스트를 해보겠습니다.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"오늘은 버트를 이용하여 테스트를 해보겠습니다.\")))\n",
    "batch.append(tokenizer.encode(\"조인트 모델이 잘 돌아가는지 궁금하군요.\"))\n",
    "valid_length.append(len(tokenizer.encode(\"조인트 모델이 잘 돌아가는지 궁금하군요.\")))\n",
    "\n",
    "\n",
    "maxlen=max(valid_length)\n",
    "input_ids = [sen+[0]*(maxlen-len(sen)) for sen in batch]\n",
    "input_ids= torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[53,\n",
       "   26,\n",
       "   174,\n",
       "   244,\n",
       "   128,\n",
       "   134,\n",
       "   130,\n",
       "   128,\n",
       "   165,\n",
       "   161,\n",
       "   233,\n",
       "   103,\n",
       "   245,\n",
       "   128,\n",
       "   165,\n",
       "   124],\n",
       "  [233,\n",
       "   128,\n",
       "   161,\n",
       "   256,\n",
       "   248,\n",
       "   128,\n",
       "   165,\n",
       "   234,\n",
       "   66,\n",
       "   66,\n",
       "   214,\n",
       "   150,\n",
       "   190,\n",
       "   211,\n",
       "   173,\n",
       "   124],\n",
       "  [53, 101, 128, 198, 103, 128, 66, 16, 103, 244, 128, 173, 55, 244, 146, 87]],\n",
       " 0.004704668186604977,\n",
       " tensor([[ 0.0967,  0.0642,  0.3189, -0.1460,  0.2150,  0.2407, -0.2924,  0.0133,\n",
       "          -0.0249, -0.0041, -0.1298, -0.3748,  0.0901, -0.1452,  0.0431,  0.0045,\n",
       "          -0.3683, -0.4025,  0.2652, -0.1089, -0.0740,  0.3377,  0.4195,  0.2688,\n",
       "          -0.1537, -0.6224, -0.5605,  0.2107, -0.1776,  0.3735, -0.5674,  0.3486,\n",
       "           0.1657, -0.4137,  0.0564,  0.1058,  0.3253,  0.5036,  0.6185, -0.0107,\n",
       "           0.1889, -0.6330,  0.2323, -0.0898,  0.0166,  0.1886, -0.3348, -0.0582,\n",
       "           0.3292, -0.3958, -0.3806,  0.1372, -0.1704, -0.1735, -0.0093, -0.1659,\n",
       "          -0.0498, -0.1903,  0.1411, -0.0853, -0.0279,  0.0663, -0.2782,  0.1782,\n",
       "          -0.2126,  0.2321,  0.2851,  0.3721, -0.3064,  0.2010,  0.2598,  0.1949],\n",
       "         [ 0.0051,  0.0400, -0.1443, -0.3463,  0.4070, -0.0230,  0.2741, -0.5778,\n",
       "           0.0284, -0.1665, -0.4894, -0.2814, -0.3305, -0.2283,  0.1862,  0.3194,\n",
       "          -0.1465, -0.6867, -0.3947, -0.0019,  0.2365,  0.1838,  0.2951,  0.0854,\n",
       "          -0.3401, -0.3723, -0.6355, -0.0707, -0.3521,  0.2738, -0.6845,  0.2138,\n",
       "           0.2980, -0.1767,  0.1400,  0.6401,  0.0219,  0.3231,  0.4231, -0.2575,\n",
       "           0.6347, -0.3883,  0.3459,  0.0917, -0.0741,  0.1417, -0.1543,  0.2607,\n",
       "           0.0821, -0.2657,  0.0942,  0.5548,  0.2553, -0.1277, -0.0941, -0.2167,\n",
       "          -0.2341, -0.0426, -0.0895, -0.2829, -0.1583,  0.0320, -0.0027,  0.1411,\n",
       "          -0.1041,  0.3587,  0.0098,  0.4555, -0.0836,  0.4264,  0.5047,  0.4365],\n",
       "         [-0.3522, -0.4118,  0.1217, -0.0494,  0.2373,  0.2366, -0.0768, -0.4555,\n",
       "          -0.1229, -0.0815, -0.0845, -0.2888, -0.1423, -0.6207,  0.5638,  0.3728,\n",
       "          -0.4532, -0.7305,  0.2210,  0.0465,  0.2660, -0.6223,  0.1970,  0.4338,\n",
       "          -0.2034, -0.2324, -0.5386,  0.0482, -0.5321,  0.3479, -0.2472,  0.0185,\n",
       "           0.3888, -0.1562,  0.3946,  0.6153,  0.0397,  0.1129,  0.3249, -0.1856,\n",
       "           0.2383, -0.1529, -0.1700, -0.1359, -0.0392, -0.0362, -0.1049,  0.0634,\n",
       "           0.2890, -0.2386,  0.0678,  0.0492,  0.3899,  0.0956, -0.0400, -0.1307,\n",
       "          -0.5741,  0.0355, -0.1290, -0.1742, -0.3719,  0.1324, -0.0560,  0.0916,\n",
       "          -0.0094,  0.3685, -0.0248,  0.1700,  0.1851,  0.0234,  0.6621,  0.1114]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint = BERT_CRF_Joint(distill=True)\n",
    "joint(input_ids, valid_length, token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
