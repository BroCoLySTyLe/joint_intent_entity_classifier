{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crf import CRF\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "import constant as config\n",
    "class BERT_LSTM_Joint(nn.Module):\n",
    "    def __init__(self, config, bert=None, distill=False):\n",
    "        super(BERT_LSTM_Joint, self).__init__()\n",
    "        \n",
    "        \n",
    "        #별도의 BERT모델을 지정하지 않으면 SKT KoBERT를 Default로 지정한다. \n",
    "        self.bert = bert\n",
    "        self.distill=distill\n",
    "        if bert is None:\n",
    "            if self.distill == True:\n",
    "                self.bert = DistilBertModel.from_pretrained('monologg/distilkobert')\n",
    "            else:\n",
    "                self.bert, self.vocab  = get_pytorch_kobert_model()\n",
    "                \n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.crf_linear = nn.Linear(config.hidden_size, config.num_entity)\n",
    "        self.intent_classifier = nn.Linear(config.hidden_size, config.num_intent)\n",
    "        self.bilstm  = nn.LSTM(config.hidden_size, config.hidden_size //2, \n",
    "                               batch_first=True, bidirectional=True )\n",
    "        self.crf = CRF(num_tags=config.num_entity, batch_first=True)\n",
    "    \n",
    "    \n",
    "    #Sentence의 길이만큼만 Attention을 취하기 위해 Mask를 생성한다.\n",
    "    def get_attention_mask(self, input_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "    \n",
    "    def forward(self, input_ids, valid_length, token_type_ids, entity=None, intent=None):\n",
    "        attention_mask = self.get_attention_mask(input_ids, valid_length)\n",
    "        \n",
    "        #all_encoder_layers는 BERT의 output\n",
    "        \n",
    "\n",
    "        if self.distill==True:\n",
    "            outputs = self.bert(input_ids=input_ids.long(), \n",
    "                                attention_mask=attention_mask) # (batch, maxlen, hidden)\n",
    "            \n",
    "            all_encoder_layers, pooled_output = outputs[0], outputs[0][:,0,:]\n",
    "            \n",
    "        else:\n",
    "            all_encoder_layers, pooled_output = self.bert(input_ids=input_ids.long(),\n",
    "                                                      token_type_ids=token_type_ids,\n",
    "                                                      attention_mask=attention_mask)\n",
    "\n",
    "        # Intent\n",
    "        cls_out = pooled_output\n",
    "        # print('cls_out')\n",
    "        # print(cls_out.size())\n",
    "        cls_out_drop = self.dropout(cls_out)\n",
    "        # print('cls_out_drop')\n",
    "        # print(cls_out_drop.size())\n",
    "        logits = self.intent_classifier(cls_out_drop)\n",
    "        \n",
    "        # Entity on CRF\n",
    "        last_encoder_layer = all_encoder_layers\n",
    "        drop = self.dropout(last_encoder_layer)\n",
    "        output, hc = self.bilstm(drop)\n",
    "        linear = self.crf_linear(output)\n",
    "        tag_seq = self.crf.decode(linear)\n",
    "\n",
    "        # For training\n",
    "        if entity is not None:\n",
    "            log_likelihood = self.crf(linear, entity)       \n",
    "            return log_likelihood, tag_seq, logits\n",
    "        \n",
    "        # For inference\n",
    "        else: \n",
    "            confidence = self.crf.compute_confidence(linear, tag_seq)\n",
    "            return tag_seq, confidence, logits\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=8\n",
    "for i in range(length):\n",
    "    mask[i][:length]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4292b580d3a0496aa86753f7d660e0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=371391.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21267125f54c4e65898603636a6f9698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=77779.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from tokenization_kobert import KoBertTokenizer\n",
    ">>> tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    ">>> tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")\n",
    "['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
    ">>> tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])\n",
    "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n",
    "\n",
    ">>> from tokenization_kobert import KoBertTokenizer\n",
    ">>> tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    ">>> tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")\n",
    "['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n",
    ">>> tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=[]\n",
    "\n",
    "for i in range(3):\n",
    "    batch.append(tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids= torch.tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_mask(input_ids, valid_length):\n",
    "    attention_mask = torch.zeros_like(input_ids)\n",
    "    for i, v in enumerate(valid_length):\n",
    "        attention_mask[i][:v] = 1\n",
    "    return attention_mask.float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_attention_mask(input_ids,[4,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bf89860ca6416e96c2dd6baa07e17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=113629967.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "distill_Bert= DistilBertModel.from_pretrained('monologg/distilkobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4385,  0.1492,  0.4601,  ..., -0.6982, -0.5436, -0.4415],\n",
       "        [ 0.3146,  0.2004,  0.6604,  ..., -0.5571, -0.1527, -0.3555],\n",
       "        [ 0.3893,  0.2564,  0.4730,  ..., -0.4671, -0.4400, -0.4779]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_Bert(input_ids,mask)[0][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
